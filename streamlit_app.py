import streamlit as st
from langchain_openai import ChatOpenAI
import os
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableBranch, RunnablePassthrough
import sys
sys.path.append("../") # å°†çˆ¶ç›®å½•æ”¾å…¥ç³»ç»Ÿè·¯å¾„ä¸­
from openai_embedding import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from chat_llm import get_llm
from knowledge_chroma import creat_db
import json
from datetime import datetime
import glob

# æ£€æµ‹çŸ¥è¯†åº“æ–‡ä»¶å¤¹å’Œå‘é‡åº“ç›®å½•çš„æœ€æ–°ä¿®æ”¹æ—¶é—´
def get_latest_mtime(path):
    mtime = os.path.getmtime(path)
    return mtime

# åŠ è½½æ•°æ®åº“
def load_chroma(filepath="./database/knowledge/", 
                persist_directory='./database/chroma', 
                embedding=OpenAIEmbeddings(api_key=st.session_state.get("openai_api_key", "")), 
                state=True):
    # å¦‚æœpersist_directoryè·¯å¾„ä¸‹æ²¡æœ‰æ–‡ä»¶ï¼Œåˆ™è°ƒç”¨creat_dbç”Ÿæˆæ•°æ®åº“
    if not os.path.exists(persist_directory) or not os.listdir(persist_directory):
        print("å‘é‡æ•°æ®åº“ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºä¸­...")
        vectordb, _ = creat_db(
            folder_path=filepath, 
            embedding=embedding,
            CHUNK_SIZE=500,
            OVERLAP_SIZE=100,
            persist_directory=persist_directory,
            query='test',
            k=10,
            state=False
        )
    else:
        embedding_function = embedding
        # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
        vectordb = Chroma(
            persist_directory=persist_directory,  # å…è®¸æˆ‘ä»¬å°†persist_directoryç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š
            embedding_function=embedding_function
        )
    if state:
        file_paths = []
        for root, dirs, files in os.walk(filepath):
            for file in files:
                file_path = os.path.join(root, file)
                file_paths.append(file_path)
        if file_paths==[]:
            print("Warning: çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡ä»¶")
        else:
            print("\nçŸ¥è¯†æ¥æº: ")
            for i in file_paths:
                print(f" - {i.replace('./database/knowledge/', '')}")
            print(f"\nå‘é‡åº“ä¸­å­˜å‚¨çš„æ•°é‡ï¼š{vectordb._collection.count()}\n")
    return vectordb

def get_retriever(llm):
    filepath = "./database/knowledge/"  # çŸ¥è¯†åº“æ–‡ä»¶å¤¹è·¯å¾„
    persist_directory = './database/chroma/'  # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
    if not os.path.exists(filepath):
        os.makedirs(filepath, exist_ok=True)
    if not os.path.exists(persist_directory):
        os.makedirs(persist_directory, exist_ok=True)
    # åŠ è½½æ•°æ®åº“
    kb_mtime = get_latest_mtime(filepath)
    chroma_mtime = get_latest_mtime(persist_directory)
    # å¦‚æœçŸ¥è¯†åº“æœ‰æ›´æ–°ï¼Œåˆ™åˆ é™¤æ—§çš„å‘é‡åº“ç›®å½•
    # print(chroma_mtime - kb_mtime)
    if kb_mtime > chroma_mtime:
        import shutil
        print("çŸ¥è¯†åº“æœ‰æ›´æ–°ï¼Œæ­£åœ¨åˆ é™¤æ—§çš„å‘é‡åº“...")
        shutil.rmtree(persist_directory)
        os.makedirs(persist_directory, exist_ok=True)
    try:

        vectordb = load_chroma(filepath,
                            persist_directory, 
                            embedding=OpenAIEmbeddings(api_key=st.session_state.get("openai_api_key", "")), 
                            state=True)
    except Exception as e:
        vectordb = load_chroma(filepath,
                            persist_directory, 
                            embedding=OpenAIEmbeddings(api_key=st.session_state.get("openai_api_key", "")), 
                            state=True)
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})
    # å‹ç¼©é—®é¢˜çš„ç³»ç»Ÿ prompt
    condense_question_system_template = (
        "è¯·æ ¹æ®èŠå¤©è®°å½•å®Œå–„ç”¨æˆ·æœ€æ–°çš„é—®é¢˜ï¼Œ"
        "å¦‚æœç”¨æˆ·æœ€æ–°çš„é—®é¢˜ä¸éœ€è¦å®Œå–„åˆ™è¿”å›ç”¨æˆ·çš„é—®é¢˜ã€‚"
        )
    # æ„é€  å‹ç¼©é—®é¢˜çš„ prompt template
    condense_question_prompt = ChatPromptTemplate([
            ("system", condense_question_system_template),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ])
    # æ„é€ æ£€ç´¢æ–‡æ¡£çš„é“¾
    # RunnableBranch ä¼šæ ¹æ®æ¡ä»¶é€‰æ‹©è¦è¿è¡Œçš„åˆ†æ”¯
    retrieve_docs = RunnableBranch(
        # åˆ†æ”¯ 1: è‹¥èŠå¤©è®°å½•ä¸­æ²¡æœ‰ chat_history åˆ™ç›´æ¥ä½¿ç”¨ç”¨æˆ·é—®é¢˜æŸ¥è¯¢å‘é‡æ•°æ®åº“
        (lambda x: not x.get("chat_history", False), (lambda x: x["input"]) | retriever, ),
        # åˆ†æ”¯ 2 : è‹¥èŠå¤©è®°å½•ä¸­æœ‰ chat_history åˆ™å…ˆè®© llm æ ¹æ®èŠå¤©è®°å½•å®Œå–„é—®é¢˜å†æŸ¥è¯¢å‘é‡æ•°æ®åº“
        condense_question_prompt | llm | StrOutputParser() | retriever,
    )

    return retrieve_docs

def combine_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs["context"])

def get_qa_history_chain(model_name="Qwen/Qwen2.5-7B-Instruct"):
    temperature = st.session_state.get("temperature", 0)
    llm = get_llm(
        temperature=temperature,
        openai_api_key=st.session_state.get("openai_api_key", ""),
        base_url="https://api.siliconflow.cn/v1",
        model_name=model_name,
        streaming=True
    )
    retrieve_docs = get_retriever(llm)

    system_prompt = (
        """ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ 
        ä½ å«å°ä¸åŒå­¦ã€‚
        ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚
        å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚
        å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ï¼Œä½†æ˜¯åˆè¦ç»å¯¹è¯¦ç»†ï¼Œä¸”ä¿è¯æ­£ç¡®æ€§ã€‚
        {context}
        """
    )
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ]
    )
    qa_chain = (
        RunnablePassthrough().assign(context=combine_docs)
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    qa_history_chain = RunnablePassthrough().assign(
        context = retrieve_docs, 
        ).assign(answer=qa_chain)
    return qa_history_chain

def gen_response(chain, input, chat_history):
    response = chain.stream({
        "input": input,
        "chat_history": chat_history
    })
    for res in response:
        if "answer" in res.keys():
            yield res["answer"]

# èŠå¤©è®°å½•æ–‡ä»¶åç»Ÿä¸€ç®¡ç†
CHAT_HISTORY_DIR = "./chat_history/"
os.makedirs(CHAT_HISTORY_DIR, exist_ok=True)

def get_current_chat_filename():
    # åˆ¤æ–­æ˜¯å¦å·²æœ‰èŠå¤©å†…å®¹
    if "chat_file" not in st.session_state:
        if st.session_state.messages and len(st.session_state.messages) > 0 and st.session_state.messages[0][1].strip():
            first_msg = st.session_state.messages[0][1].strip().replace('\n', ' ').replace(' ', '_')[:20]
            fname = f"{first_msg}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        else:
            fname = f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        st.session_state.chat_file = os.path.join(CHAT_HISTORY_DIR, fname)
    return st.session_state.chat_file

def save_chat_history(messages):
    filename = get_current_chat_filename()
    # ä¿®å¤æ–°å»ºèŠå¤©æ—¶ chat_file ä¸º None çš„é—®é¢˜
    if filename is None:
        # é‡æ–°ç”Ÿæˆæ–‡ä»¶å
        if messages and len(messages) > 0 and messages[0][1].strip():
            first_msg = messages[0][1].strip().replace('\n', ' ').replace(' ', '_')[:20]
            fname = f"{first_msg}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        else:
            fname = f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filename = os.path.join(CHAT_HISTORY_DIR, fname)
        st.session_state.chat_file = filename
    # å¦‚æœæ²¡æœ‰æ¶ˆæ¯ï¼Œä¸”æ–‡ä»¶å­˜åœ¨åˆ™åˆ é™¤
    if not messages or len(messages) == 0:
        if filename and os.path.exists(filename):
            os.remove(filename)
        return None
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å‘½å
    if messages and len(messages) > 0 and messages[0][1].strip():
        first_msg = messages[0][1].strip().replace('\n', ' ').replace(' ', '_')[:20]
        expected_name = f"{first_msg}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        expected_path = os.path.join(CHAT_HISTORY_DIR, expected_name)
        if os.path.basename(filename).startswith('chat_'):
            # ä»…åœ¨é¦–æ¬¡æœ‰å†…å®¹æ—¶é‡å‘½å
            if os.path.exists(filename):
                os.rename(filename, expected_path)
                st.session_state.chat_file = expected_path
                filename = expected_path
            else:
                # å¦‚æœåŸæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥ç”¨æ–°æ–‡ä»¶åä¿å­˜
                st.session_state.chat_file = expected_path
                filename = expected_path
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(messages, f, ensure_ascii=False, indent=2)
    return filename

def list_chat_history_files():
    files = glob.glob(os.path.join(CHAT_HISTORY_DIR, "*.json"))
    files.sort(reverse=True)
    return files

def load_chat_history(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        return json.load(f)



# Streamlit åº”ç”¨ç¨‹åºç•Œé¢
def main():
    st.markdown('### ğŸ”— å°ä¸åŒå­¦å¸®ä½ åŠ')

    # ä¾§è¾¹æ åŠŸèƒ½å’Œå†å²èŠå¤©åŒºé›†æˆ
    with st.sidebar:
        st.markdown("## åŠŸèƒ½åŒº")
        st.markdown("---")
        st.markdown("#### APIå¯†é’¥ä¸æ¨¡å‹å‚æ•°")
        openai_api_key = st.text_input("è¯·è¾“å…¥ OpenAI API Key", value=st.session_state.get("openai_api_key", ""), type="password")
        st.session_state.openai_api_key = openai_api_key
        temperature = st.slider(
            label="æ¨¡å‹é£æ ¼",
            min_value=0.0,
            max_value=1.0,
            value=0.0,
            step=0.01,
            format="%.2f"
        )
        st.session_state.temperature = temperature
        st.write(f"å½“å‰æ¨¡å‹å±äºï¼š{'ç†ç§‘ç”Ÿ' if temperature<0.5 else ('æ–‡ç§‘ç”Ÿ' if temperature>0.5 else 'æ–‡ç†å…¼å¤‡')}")

        # 1. æ¨¡å‹é€‰æ‹©
        model_name_dict = {
            1: "Qwen/Qwen2.5-7B-Instruct",
            2: "Pro/Qwen/Qwen2.5-7B-Instruct",
            3: "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
            4: "Qwen/Qwen3-30B-A3B-Instruct-2507",
            5: "deepseek-ai/DeepSeek-V3.1",
            6: "Pro/deepseek-ai/DeepSeek-V3.1"
        }
        model_desc_dict = {
            1: "é»˜è®¤æ¨¡å‹ï¼Œå…è´¹ï¼Œååº”å¿«ï¼Œèƒ½åŠ›å°šå¯",
            2: "1æ¨¡å‹çš„proç‰ˆï¼Œååº”ç†è®ºä¸Šå¿«ä¸€äº›ï¼Œ1æ¨¡å‹å¡é¡¿å¯ä»¥åˆ‡æ¢ä¸º2",
            3: "deepseekå°æ¨¡å‹ï¼Œæœ‰æ€è€ƒï¼Œæˆ–è®¸ä¼šæ›´å‡†ç¡®ï¼Œä½†æ˜¯ååº”é€Ÿåº¦ä¼šå˜æ…¢",
            4: "Qwenå¤§æ¨¡å‹ï¼Œé«˜è´¨é‡è¾“å‡ºå¯é€‰",
            5: "deepseekå¤§æ¨¡å‹ï¼Œèƒ½åŠ›ç†è®ºä¸Šæ¯”4å¼º",
            6: "5æ¨¡å‹çš„proç‰ˆï¼Œååº”æ›´å¿«ï¼Œèƒ½åŠ›ä¸å˜ï¼Œæ…ç”¨ï¼ˆè¶…çº§è´µï¼‰"
        }
        model_options = [f"{i}: {model_name_dict[i]}" for i in model_name_dict]
        selected_model_idx = st.selectbox("é€‰æ‹©æ¨¡å‹ï¼š", options=list(model_name_dict.keys()), format_func=lambda x: f"{model_name_dict[x]}")
        st.session_state.selected_model = model_name_dict[selected_model_idx]
        st.info(f"æ¨¡å‹æè¿°ï¼š{model_desc_dict[selected_model_idx]}")

        # 2. PDFä¸Šä¼ 
        st.markdown("---")
        st.markdown("### ä¸Šä¼ PDFæ–‡ä»¶")
        uploaded_file = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", type=["pdf"])
        pdf_save_path = "./database/knowledge/"
        if not os.path.exists(pdf_save_path):
            os.makedirs(pdf_save_path, exist_ok=True)
        if uploaded_file is not None:
            if uploaded_file.type != "application/pdf":
                st.error("ä»…æ”¯æŒPDFæ–‡ä»¶ï¼")
            else:
                save_path = os.path.join(pdf_save_path, uploaded_file.name)
                with open(save_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                st.success(f"æ–‡ä»¶ {uploaded_file.name} å·²ä¿å­˜åˆ°çŸ¥è¯†åº“ç›®å½•ï¼")

        # 3. ç”ŸæˆçŸ¥è¯†åº“æŒ‰é’®
        if st.button("ç”ŸæˆçŸ¥è¯†åº“"):
            pdf_files = [os.path.join(pdf_save_path, f) for f in os.listdir(pdf_save_path) if f.endswith('.pdf')]
            if not pdf_files:
                st.warning("çŸ¥è¯†åº“ç›®å½•ä¸‹æ²¡æœ‰PDFæ–‡ä»¶ï¼Œæ— æ³•ç”ŸæˆçŸ¥è¯†åº“ï¼")
            else:
                st.info("æ­£åœ¨ç”ŸæˆçŸ¥è¯†åº“ï¼Œè¯·ç¨å€™...")
                for pdf_file in pdf_files:
                    creat_db(
                        folder_path=pdf_save_path,
                        CHUNK_SIZE=500,
                        OVERLAP_SIZE=100,
                        embedding=OpenAIEmbeddings(api_key=st.session_state.get("openai_api_key", "")),
                        persist_directory='./database/chroma',
                        query='test',
                        k=10,
                        state=False
                    )
                st.success("çŸ¥è¯†åº“å·²ç”Ÿæˆï¼")

        # 4. æ˜¾ç¤ºçŸ¥è¯†åº“ä¸­çš„PDFæ–‡ä»¶åˆ—è¡¨
        st.markdown("---")
        st.markdown("### å½“å‰çŸ¥è¯†åº“PDFæ–‡ä»¶åˆ—è¡¨")
        pdf_files_list = [f for f in os.listdir(pdf_save_path) if f.endswith('.pdf')]
        if pdf_files_list:
            for pdf_file in pdf_files_list:
                st.write(f"- {pdf_file}")
        else:
            st.info("çŸ¥è¯†åº“ç›®å½•ä¸‹æš‚æ— PDFæ–‡ä»¶ã€‚")

        # 5. å†å²èŠå¤©è®°å½•åŒº
        st.markdown("---")
        st.markdown('## å†å²èŠå¤©è®°å½•')
        chat_files = list_chat_history_files()
        if chat_files:
            for chat_file in chat_files:
                chat_title = os.path.basename(chat_file).replace('.json','')
                if st.button(chat_title, key=f"load_{chat_title}"):
                    if st.session_state.get('chat_file') != chat_file:
                        st.session_state.messages = load_chat_history(chat_file)
                        st.session_state.chat_file = chat_file
        else:
            st.info("æš‚æ— å†å²èŠå¤©è®°å½•ã€‚")

        

    # ç”¨äºè·Ÿè¸ªå¯¹è¯å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []
    # å­˜å‚¨æ£€ç´¢é—®ç­”é“¾
    if "qa_history_chain" not in st.session_state or st.session_state.get("qa_chain_model_name") != st.session_state.selected_model:
        st.session_state.qa_history_chain = get_qa_history_chain(model_name=st.session_state.selected_model)
        st.session_state.qa_chain_model_name = st.session_state.selected_model

    # ä¸»é¡µé¢åªä¿ç•™èŠå¤©åŒºç›¸å…³å†…å®¹

    if st.button("æ–°å»ºèŠå¤©", key="new_chat"):
        st.session_state.messages = []
        st.session_state.chat_file = None
    
    chat_container = st.container()
    with chat_container:
        st.info("æˆ‘æ˜¯ä½ çš„çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ ï¼Ÿ")
        # å±•ç¤ºå†å²æ¶ˆæ¯ï¼ŒAIæ¶ˆæ¯å¢åŠ å¤åˆ¶æŒ‰é’®
        for idx, message in enumerate(st.session_state.messages):
            with st.chat_message(message[0]):
                if message[0] == "ai":
                    st.write(message[1])
                    # å¤åˆ¶æŒ‰é’®
                    if st.button("ğŸ“‹", key=f"copy_{idx}"):
                        # st.session_state["copy_text"] = message[1]
                        st.code(message[1], language=None)
                else:
                    st.write(message[1])
    prompt = st.chat_input("Say something")
    
    if prompt:
        st.session_state.messages.append(("human", prompt))
        with st.chat_message("human"):
            st.write(prompt)
        answer = gen_response(
            chain=st.session_state.qa_history_chain,
            input=prompt,
            chat_history=st.session_state.messages
        )
        with st.chat_message("ai"):
            output = st.write_stream(answer)
            if st.button("ğŸ“‹", key="new_copy"):
                # st.session_state["copy_text"] = message[1]
                st.code(output, language=None)
        st.session_state.messages.append(("ai", output))
        save_chat_history(st.session_state.messages)
    # with col_history:

if __name__ == "__main__":
    
    main()



